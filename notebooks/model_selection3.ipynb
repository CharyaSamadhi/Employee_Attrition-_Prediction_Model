{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96001616-97c5-4cf0-b5f2-e3697abdb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression (class_weight=balanced) (Cost-Sensitive) ===\n",
      "Chosen threshold (val F1): 0.4866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.20      0.32     17762\n",
      "           1       0.10      0.81      0.18      1976\n",
      "\n",
      "    accuracy                           0.26     19738\n",
      "   macro avg       0.50      0.50      0.25     19738\n",
      "weighted avg       0.82      0.26      0.31     19738\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3494 14268]\n",
      " [  382  1594]]\n",
      "\n",
      "=== Random Forest (class_weight=balanced) (Cost-Sensitive) ===\n",
      "Chosen threshold (val F1): 0.0400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.00      0.00     17762\n",
      "           1       0.10      1.00      0.18      1976\n",
      "\n",
      "    accuracy                           0.10     19738\n",
      "   macro avg       0.48      0.50      0.09     19738\n",
      "weighted avg       0.77      0.10      0.02     19738\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   34 17728]\n",
      " [    6  1970]]\n",
      "\n",
      "=== Gradient Boosting (no class_weight) (Cost-Sensitive) ===\n",
      "Chosen threshold (val F1): 0.0812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.05      0.10     17762\n",
      "           1       0.10      0.94      0.18      1976\n",
      "\n",
      "    accuracy                           0.14     19738\n",
      "   macro avg       0.49      0.50      0.14     19738\n",
      "weighted avg       0.81      0.14      0.11     19738\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  931 16831]\n",
      " [  117  1859]]\n",
      "\n",
      "=== XGBoost (scale_pos_weight) (Cost-Sensitive) ===\n",
      "Chosen threshold (val F1): 0.3550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.10      0.19     17762\n",
      "           1       0.10      0.89      0.18      1976\n",
      "\n",
      "    accuracy                           0.18     19738\n",
      "   macro avg       0.50      0.50      0.18     19738\n",
      "weighted avg       0.82      0.18      0.19     19738\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1855 15907]\n",
      " [  217  1759]]\n",
      "\n",
      "=== Cost-Sensitive Models: Comparison ===\n",
      "                                              Accuracy  Balanced_Acc  \\\n",
      "Model                                                                 \n",
      "Logistic Regression (class_weight=balanced)  0.257777      0.501696   \n",
      "Random Forest (class_weight=balanced)        0.101530      0.499439   \n",
      "Gradient Boosting (no class_weight)          0.141352      0.496602   \n",
      "XGBoost (scale_pos_weight)                   0.183099      0.497309   \n",
      "\n",
      "                                             Precision    Recall        F1  \\\n",
      "Model                                                                        \n",
      "Logistic Regression (class_weight=balanced)   0.100492  0.806680  0.178720   \n",
      "Random Forest (class_weight=balanced)         0.100010  0.996964  0.181785   \n",
      "Gradient Boosting (no class_weight)           0.099465  0.940789  0.179909   \n",
      "XGBoost (scale_pos_weight)                    0.099570  0.890182  0.179106   \n",
      "\n",
      "                                              ROC_AUC    PR_AUC  Threshold  \n",
      "Model                                                                       \n",
      "Logistic Regression (class_weight=balanced)  0.496942  0.098572   0.486568  \n",
      "Random Forest (class_weight=balanced)        0.509500  0.105479   0.040000  \n",
      "Gradient Boosting (no class_weight)          0.505122  0.100871   0.081226  \n",
      "XGBoost (scale_pos_weight)                   0.497494  0.098254   0.354962  \n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SCRIPT 1: COST-SENSITIVE LEARNING (NO RESAMPLING)\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, balanced_accuracy_score,\n",
    "    classification_report, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Load artifacts/data\n",
    "preprocessor = joblib.load(\"preprocessor.pkl\")\n",
    "X_train, X_test, y_train, y_test = joblib.load(\"splits.pkl\")\n",
    "\n",
    "# --- Preprocess (fit on train only, transform both)\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "# --- Make validation split from train (stratified)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_proc, y_train, test_size=0.2,\n",
    "    random_state=RANDOM_STATE, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- Compute pos_weight for XGBoost\n",
    "cnt = Counter(y_tr)\n",
    "neg, pos = cnt[0], cnt[1]\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "\n",
    "# --- Define cost-sensitive models (no SMOTE)\n",
    "log_reg_cs = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "rf_cs      = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1, class_weight=\"balanced\")\n",
    "gb_cs      = GradientBoostingClassifier(random_state=RANDOM_STATE)  # no class_weight\n",
    "xgb_cs     = XGBClassifier(\n",
    "    n_estimators=300, learning_rate=0.1, max_depth=4,\n",
    "    subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE, scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "def evaluate_with_f1_threshold(model, name):\n",
    "    \"\"\"\n",
    "    Train on ORIGINAL imbalanced train fold (no resampling).\n",
    "    Choose a threshold on validation that maximizes F1.\n",
    "    Evaluate on test with that threshold. Return metrics row.\n",
    "    \"\"\"\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # --- tune threshold on validation by F1 ---\n",
    "    val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_val, val_proba)\n",
    "    f1s = (2 * precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "    best_idx = np.argmax(f1s[:-1])  # last P/R point has no threshold\n",
    "    best_thr = float(thresholds[best_idx])\n",
    "\n",
    "    # --- evaluate on test ---\n",
    "    test_proba = model.predict_proba(X_test_proc)[:, 1]\n",
    "    y_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "    acc   = accuracy_score(y_test, y_pred)\n",
    "    bal   = balanced_accuracy_score(y_test, y_pred)\n",
    "    prec  = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec   = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1    = f1_score(y_test, y_pred, zero_division=0)\n",
    "    roc   = roc_auc_score(y_test, test_proba)\n",
    "    prauc = average_precision_score(y_test, test_proba)\n",
    "\n",
    "    print(f\"\\n=== {name} (Cost-Sensitive) ===\")\n",
    "    print(f\"Chosen threshold (val F1): {best_thr:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Threshold\": best_thr,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Balanced_Acc\": bal,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "        \"ROC_AUC\": roc,\n",
    "        \"PR_AUC\": prauc\n",
    "    }\n",
    "\n",
    "# --- Run/evaluate\n",
    "results_cs = []\n",
    "results_cs.append(evaluate_with_f1_threshold(log_reg_cs, \"Logistic Regression (class_weight=balanced)\"))\n",
    "results_cs.append(evaluate_with_f1_threshold(rf_cs,      \"Random Forest (class_weight=balanced)\"))\n",
    "results_cs.append(evaluate_with_f1_threshold(gb_cs,      \"Gradient Boosting (no class_weight)\"))\n",
    "results_cs.append(evaluate_with_f1_threshold(xgb_cs,     \"XGBoost (scale_pos_weight)\"))\n",
    "\n",
    "# --- Table\n",
    "results_cs_df = pd.DataFrame(results_cs).set_index(\"Model\")\n",
    "print(\"\\n=== Cost-Sensitive Models: Comparison ===\\n\",\n",
    "      results_cs_df[[\"Accuracy\",\"Balanced_Acc\",\"Precision\",\"Recall\",\"F1\",\"ROC_AUC\",\"PR_AUC\",\"Threshold\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de370b99-525f-428c-877e-4368226a6346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
